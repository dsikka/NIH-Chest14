{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgYI7rKAuaZU"
   },
   "outputs": [],
   "source": [
    "# Load in PyTorch's pretrained network\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as func\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOAUwnbQ98B1"
   },
   "outputs": [],
   "source": [
    "class UtilFuncs():\n",
    "    def plot_losses(self):\n",
    "        return 0\n",
    "    def plot_images(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add separate transformations for testing and training \n",
    "# Augmentatiosn for data?\n",
    "class CHXData(Dataset):\n",
    "    def __init__(self, img_dir, path_to_labels):\n",
    "        self.img_dir = img_dir\n",
    "        self.data_df = pd.read_csv(path_to_labels)\n",
    "        self.img_names = self.data_df['Image Index']\n",
    "        self.labels = np.asarray(self.data_df.loc[:, self.data_df.columns != 'Image Index'])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(512),\n",
    "            # Should the normalization be from the statistics of the training set or the entire set?\n",
    "            # Normalize by using images specific to this domain,\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.50546, 0.50546, 0.50546), (0.2319, 0.2319, 0.2319))\n",
    "        ])\n",
    "        \n",
    "    # Return size of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        # Converts to 3-Channel as Resnet takes in 3 channels\n",
    "        # seems to repeat the same values for every channel\n",
    "        image = self.transform(Image.open(img_path).convert('RGB'))\n",
    "        label = self.labels[idx]\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kAoxFA7vrdfh"
   },
   "outputs": [],
   "source": [
    "class CHXModel():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.losses = None\n",
    "        self.optimizer = None\n",
    "    \n",
    "    # Freeze or un-freeze model layers as required\n",
    "    def update_grad(self, grad_val):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = grad_val\n",
    "            \n",
    "    # Initial model setup\n",
    "    def set_up_model(self, n_classes, lr):\n",
    "        self.model = models.resnet34(pretrained=True, progress=True)\n",
    "        # Freeze all layers\n",
    "        self.update_grad(False)\n",
    "        # Resnet has one fully connected layer, which outputs dimensions of n_classes\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = torch.nn.Linear(num_ftrs, n_classes)\n",
    "        self.model.cuda()\n",
    "        # Use a binary cross-entropy loss function; Applies sigmoid internally (generating probabilities)\n",
    "        # On the probabilities, cross-entropy loss is computed\n",
    "        # Because we are doing multilabel, this is better as the value outtputted (unlike softmax)\n",
    "        # is independent of the other values (while in softmax, probabilities must add up to one)\n",
    "        self.losses =  torch.nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "    \n",
    "    def load_checkpoint(self, file_path):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.model.load_state_dict(checkpoint['state'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        return checkpoint['epoch']\n",
    "\n",
    "    def save_checkpoint(self, state, filename):\n",
    "        torch.save(state, filename)\n",
    "    \n",
    "    def evaluate(self, testloader):\n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "        # Don't store intermediate values which would be needed for backpropgation (memory saving)\n",
    "        with torch.no_grad():\n",
    "            # Go through each batch in the testloader\n",
    "            for X, y in testloader:\n",
    "                input_img = Variable(X.cuda(non_blocking=True))\n",
    "                labels = Variable(y.float().cuda(non_blocking=True))\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(input_img)\n",
    "                print(output.shape)\n",
    "    \n",
    "    # Add in any data augmentation while training?\n",
    "    def train(self, epochs, trainloader, val_loader, checkpoint_path=None):\n",
    "        # Empty cache before training\n",
    "        torch.cuda.empty_cache()\n",
    "        if checkpoint_path != None:\n",
    "            current_epoch = self.load_checkpoint(checkpoint_path)\n",
    "            epochs = epochs - current_epoch\n",
    "            \n",
    "        for e in range(epochs):\n",
    "            # Get loss per epoch\n",
    "            running_loss = 0\n",
    "            for i, (X, y) in enumerate(trainloader):\n",
    "                input_img = Variable(X.cuda(non_blocking=True))\n",
    "                labels = Variable(y.float().cuda(non_blocking=True))\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(input_img)\n",
    "                loss = self.losses(output, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss = running_loss + loss.item()\n",
    "            # Find the loss for the current epoch\n",
    "            loss = running_loss/len(trainloader)\n",
    "            print(f\"Epoch {e + 1} - Loss: {loss}\")\n",
    "            state = {\n",
    "                'epoch': e + 1,\n",
    "                'state': self.model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict()\n",
    "            }\n",
    "            checkpoint_path = os.path.join('./checkpoints/checkpoint_epoch_' + str(e + 1) + '.pth.tar') \n",
    "            self.save_checkpoint(state, checkpoint_path)\n",
    "            #self.evaluate(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Yoo7fvtj0df"
   },
   "outputs": [],
   "source": [
    "n_classes = 15\n",
    "learning_rate = 1e-4\n",
    "chx_model = CHXModel()\n",
    "chx_model.set_up_model(n_classes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4Hp73LxjlfVB",
    "outputId": "311615fe-95f9-42ca-918e-c3f02f14400f"
   },
   "outputs": [],
   "source": [
    "path_to_train_data = './data/Train/'\n",
    "path_to_test_data = './data/Test/'\n",
    "path_to_val_data = './data/Val'\n",
    "path_to_test_labels = './data/Labels/Test_Labels.csv'\n",
    "path_to_train_labels = './data/Labels/Train_Labels.csv'\n",
    "path_to_val_labels='./data/Labels/Val_Labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and data loader\n",
    "test_dataset = CHXData(path_to_test_data, path_to_test_labels)\n",
    "train_dataset = CHXData(path_to_train_data, path_to_train_labels)\n",
    "val_dataset =  CHXData(path_to_val_data, path_to_val_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "it = iter(test_loader)\n",
    "for i in range(10):\n",
    "    # Get a random index value for an index in the batch\n",
    "    idx = np.random.randint(0, 99, 1)[0]\n",
    "    # Use the random index to get an image, label in the batch\n",
    "    image, label = [x[idx] for x in next(it)]\n",
    "    plt.figure(num=None, figsize=(8, 6))\n",
    "    plt.imshow(image.numpy().reshape(512, 512));\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.2166028259577892\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/checkpoints/checkpoint_epoch_1.pth.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-56d73e6ba8d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train with the layers frozen for 5 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchx_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-aafb21c6baa3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, trainloader, val_loader, checkpoint_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m             }\n\u001b[1;32m     78\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/checkpoints/checkpoint_epoch_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-aafb21c6baa3>\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, state, filename)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \"\"\"\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/checkpoints/checkpoint_epoch_1.pth.tar'"
     ]
    }
   ],
   "source": [
    "# Train with the layers frozen for 5 epochs\n",
    "chx_model.train(5, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO ADD DATA AUGMENTATION WHEN TRAINING\n",
    "# NEED TO DEAL WITH IMBALANCE DATA\n",
    "checkpoint_path = os.path.join('./checkpoints/checkpoint_epoch_' + str(0 + 1) + '.pth.tar')\n",
    "state = {'epoch': 0 + 1, 'state': chx_model.model.state_dict(), 'optimizer': chx_model.optimizer.state_dict()}\n",
    "chx_model.save_checkpoint(state, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
