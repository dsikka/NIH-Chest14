{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgYI7rKAuaZU"
   },
   "outputs": [],
   "source": [
    "# Load in PyTorch's pretrained network\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as func\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sklearn.metrics as sk_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yOAUwnbQ98B1"
   },
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "def plot_losses(losses):\n",
    "    plt.scatter(range(len(losses)), losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Losses')\n",
    "    plt.show()\n",
    "    \n",
    "# Plot 10 random images given a data loader with their ground truth values\n",
    "# Modify function to just plot an image when training and testing?\n",
    "def plot_images(dataloader):\n",
    "    classes = ['Cardiomegaly', 'Emphysema', 'Effusion', 'No Finding', 'Hernia',\n",
    "       'Infiltration', 'Mass', 'Nodule', 'Atelectasis', 'Pneumothorax',\n",
    "       'Pleural_Thickening', 'Pneumonia', 'Fibrosis', 'Edema',\n",
    "       'Consolidation']\n",
    "    it = iter(dataloader)\n",
    "    for i in range(10):\n",
    "        idx = np.random.randint(0, 99, 1)[0]\n",
    "        image, label = [x[idx] for x in next(it)]\n",
    "        plt.figure(num=None, figsize=(8, 6))\n",
    "        plt.imshow(image)\n",
    "        correct_labels = [classes[idx] for (idx, val) in enumerate(vals) if val == 1]\n",
    "        correct_labels = \", \".join(correct_labels)\n",
    "        print(f'Correct Labels: {correct_labels}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add separate transformations for testing and training \n",
    "# Augmentatiosn for data?\n",
    "class CHXData(Dataset):\n",
    "    def __init__(self, img_dir, path_to_labels, is_trainset):\n",
    "        self.img_dir = img_dir\n",
    "        self.data_df = pd.read_csv(path_to_labels)\n",
    "        self.img_names = self.data_df['Image Index']\n",
    "        self.labels = np.asarray(self.data_df.loc[:, self.data_df.columns != 'Image Index'])\n",
    "        \n",
    "        # Normalization and data augmentation\n",
    "        self.train_transforms = transforms.Compose([\n",
    "            transforms.Resize(512),\n",
    "            # Should the normalization be from the statistics of the training set or the entire set?\n",
    "            # Normalize by using images specific to this domain,\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(180),\n",
    "            transforms.RandomCrop(384),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.50546, 0.50546, 0.50546), (0.2319, 0.2319, 0.2319))\n",
    "        ])\n",
    "        self.test_transforms = transforms.Compose([\n",
    "            transforms.Resize(512),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.50546, 0.50546, 0.50546), (0.2319, 0.2319, 0.2319))\n",
    "        ])\n",
    "        self.is_train = is_trainset\n",
    "        \n",
    "    # Return size of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        # Converts to 3-Channel as Resnet takes in 3 channels\n",
    "        # seems to repeat the same values for every channel\n",
    "        if self.is_train:\n",
    "            image = self.train_transforms(Image.open(img_path).convert('RGB'))\n",
    "        else:\n",
    "            image = self.test_transforms(Image.open(img_path).convert('RGB'))\n",
    "        label = self.labels[idx]\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kAoxFA7vrdfh"
   },
   "outputs": [],
   "source": [
    "class CHXModel():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.losses = None\n",
    "        self.optimizer = None\n",
    "        self.model_losses = None\n",
    "    \n",
    "    # Freeze or un-freeze model layers as required\n",
    "    def update_grad(self, grad_val):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = grad_val\n",
    "            \n",
    "    # Initial model setup\n",
    "    def set_up_model(self, n_classes, lr):\n",
    "        self.model = models.resnet34(pretrained=True, progress=True)\n",
    "        # Freeze all layers\n",
    "        self.update_grad(False)\n",
    "        # Resnet has one fully connected layer, which outputs dimensions of n_classes\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = torch.nn.Linear(num_ftrs, n_classes)\n",
    "        self.model.cuda()\n",
    "        # Use a binary cross-entropy loss function; Applies sigmoid internally (generating probabilities)\n",
    "        # On the probabilities, cross-entropy loss is computed\n",
    "        # Because we are doing multilabel, this is better as the value outtputted (unlike softmax)\n",
    "        # is independent of the other values (while in softmax, probabilities must add up to one)\n",
    "        self.losses =  torch.nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "    \n",
    "    def load_checkpoint(self, file_path):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.model.load_state_dict(checkpoint['state'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        return checkpoint['epoch']\n",
    "\n",
    "    def save_checkpoint(self, state, filename):\n",
    "        torch.save(state, filename)\n",
    "    \n",
    "    # F1 score? Something else that isn't accuracy as we have imbalanced data and because this is medicine\n",
    "    def evaluate(self, testloader):\n",
    "        with torch.no_grad():\n",
    "            # Go through each batch in the testloader\n",
    "            for X, y in testloader:\n",
    "                input_img = X.cuda(non_blocking=True)\n",
    "                labels = y.float().cuda(non_blocking=True)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(input_img)\n",
    "                sig = torch.nn.Sigmoid()\n",
    "                probabilities = sig(output)\n",
    "                # Check if predictions are greater than 0.5\n",
    "                predictions = probabilities >= 0.75\n",
    "                # Implement F1 score properly, batch wise, something else?\n",
    "                \n",
    "    \n",
    "    def train(self, epochs, trainloader, val_loader, checkpoint_path=None):\n",
    "        # Empty cache before training\n",
    "        torch.cuda.empty_cache()\n",
    "        if checkpoint_path != None:\n",
    "            start_epoch = self.load_checkpoint(checkpoint_path)\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "        for e in range(start_epoch, epochs):\n",
    "            # Get loss per epoch\n",
    "            running_loss = 0\n",
    "            for i, (X, y) in enumerate(trainloader):\n",
    "                input_img = X.cuda(non_blocking=True)\n",
    "                labels = y.float().cuda(non_blocking=True)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(input_img)\n",
    "                loss = self.losses(output, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss = running_loss + loss.item()\n",
    "            # Find the loss for the current epoch\n",
    "            loss = running_loss/len(trainloader)\n",
    "            print(f\"Epoch {e + 1} - Loss: {loss}\")\n",
    "            # Save model losses so far so we can plot them if we stop half way?\n",
    "            state = {\n",
    "                'epoch': e + 1,\n",
    "                'state': self.model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict()\n",
    "            }\n",
    "            checkpoint_path = os.path.join('./checkpoints/checkpoint_epoch_' + str(e + 1) + '.pth.tar') \n",
    "            self.save_checkpoint(state, checkpoint_path)\n",
    "            #self.evaluate(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Yoo7fvtj0df"
   },
   "outputs": [],
   "source": [
    "n_classes = 15\n",
    "learning_rate = 1e-4\n",
    "chx_model = CHXModel()\n",
    "chx_model.set_up_model(n_classes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4Hp73LxjlfVB",
    "outputId": "311615fe-95f9-42ca-918e-c3f02f14400f"
   },
   "outputs": [],
   "source": [
    "path_to_train_data = './data/Train/'\n",
    "path_to_test_data = './data/Test/'\n",
    "path_to_val_data = './data/Val'\n",
    "path_to_test_labels = './data/Labels/Test_Labels.csv'\n",
    "path_to_train_labels = './data/Labels/Train_Labels.csv'\n",
    "path_to_val_labels='./data/Labels/Val_Labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and data loader\n",
    "test_dataset = CHXData(path_to_test_data, path_to_test_labels, False)\n",
    "train_dataset = CHXData(path_to_train_data, path_to_train_labels, True)\n",
    "val_dataset =  CHXData(path_to_val_data, path_to_val_labels, False)\n",
    "\n",
    "# More num_workers consumes more memory for good for speeding up I/O\n",
    "# pin_memory=True enables fast data transfer to GPUs\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, pin_memory=True,)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.2155927475900491\n",
      "Epoch 2 - Loss: 0.1934442875933127\n",
      "Epoch 3 - Loss: 0.1909137600231844\n"
     ]
    }
   ],
   "source": [
    "# Train with the layers frozen for 5 epochs\n",
    "# Very slow still? Non_blocking, clearing cache, pin memory?\n",
    "chx_model.train(5, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything from the dataloader will be augmented even if im just plotting random exmaples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO DEAL WITH IMBALANCE DATA\n",
    "# Update evaluate function to give accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.19903892925217154\n",
      "Epoch 2 - Loss: 0.19446507664409315\n",
      "Epoch 3 - Loss: 0.1921391779214947\n",
      "Epoch 4 - Loss: 0.1906038391689587\n",
      "Epoch 5 - Loss: 0.1893047764610418\n"
     ]
    }
   ],
   "source": [
    "chx_model.train(5, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
